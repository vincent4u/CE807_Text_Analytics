{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgzonotRldmB2EbydpHQDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/CE807_Text_Analytics/blob/main/check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cWozEkz6hh4",
        "outputId": "29314a6e-cef8-4716-9b5d-f7046fa225c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE3dZl4E6j-t",
        "outputId": "68069d04-08b8-4727-cbfb-d2effa086303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LrUYuUFD6nss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_id = 2314555\n",
        "np.random.seed(student_id)\n",
        "\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "#train_df.head(20)\n",
        "\n",
        "val_df=pd.read_csv('/content/valid.csv')\n",
        "#val_df.head(20)\n",
        "\n",
        "test_df=pd.read_csv('/content/test.csv')\n",
        "#test_df.head(20)"
      ],
      "metadata": {
        "id": "fQHjDu5m8fQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove specific tokens\n",
        "    text = text.replace(\"SDATA_5 :\", \"\").replace(\"EDATA_5\", \"\").replace(\"NEWLINE_TOKEN\", \"\")\n",
        "\n",
        "\n",
        "    # Remove special characters and punctuation marks using regular expressions\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove URLs, email addresses, and phone numbers\n",
        "    text = re.sub(r\"http\\S+|www\\S+|\\S+@\\S+|\\d{10}\", \"\", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    # Remove non-ASCII characters\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra white spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_text = \" \".join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Preprocess the text\n",
        "train_df['preprocessed_comment'] = train_df['comment'].apply(preprocess_text)\n",
        "val_df['preprocessed_comment'] = val_df['comment'].apply(preprocess_text)\n",
        "\n",
        "print(train_df['preprocessed_comment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSYeCA-Q-h4y",
        "outputId": "05710695-0c2c-4206-ec7b-05aad8a8c3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       kindergarden ignorantsve added pakistani artic...\n",
            "1                   violate rrv rule may banned wikipedia\n",
            "2                     judging message tp could use advice\n",
            "3                                     love fatty sex mmmm\n",
            "4        please could explain thinking warning page thank\n",
            "                              ...                        \n",
            "8694    utci beg differ history hugely important licen...\n",
            "8695           penis sex grawp haeppenispenisvaginavagina\n",
            "8696                      thats exactly gamer dying breed\n",
            "8697                          article unless fact support\n",
            "8698                milton friedman lp also measure thing\n",
            "Name: preprocessed_comment, Length: 8699, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "#good\n",
        "# best_params = {\n",
        "#     \"objective\": \"reg:squarederror\",\n",
        "#     \"n_estimators\": 649,\n",
        "#     \"verbosity\": 0,\n",
        "#     \"learning_rate\": 0.029624377829993845,\n",
        "#     \"max_depth\": 5,\n",
        "#     \"subsample\":  0.9815939695298614,\n",
        "#     \"colsample_bytree\": 0.5082728972365318,\n",
        "#     \"min_child_weight\": 8\n",
        "# }\n",
        "best_params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"n_estimators\": 750,\n",
        "    \"verbosity\": 0,\n",
        "    \"learning_rate\": 0.009624377829993845,\n",
        "    \"max_depth\": 9,\n",
        "    \"subsample\":  0.9815939695298614,\n",
        "    \"colsample_bytree\": 0.5082728972365318,\n",
        "    \"min_child_weight\": 8\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Create an XGBRegressor model with the best hyperparameters\n",
        "#vectorizer = TfidfVectorizer()\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer on the training data and transform the training data\n",
        "train_features = vectorizer.fit_transform(train_df['preprocessed_comment'])\n",
        "\n",
        "# Transform the validation data using the fitted vectorizer\n",
        "val_features = vectorizer.transform(val_df['preprocessed_comment'])\n",
        "\n",
        "# Create an instance of XGBClassifier with the best parameters\n",
        "model = XGBClassifier(**best_params, tree_method='hist')\n",
        "\n",
        "# Fit the model on the training features and labels\n",
        "model.fit(train_features, train_df['toxicity'])\n",
        "\n",
        "# Predict on the validation features\n",
        "val_pred = model.predict(val_features)\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(val_df['toxicity'], val_pred)\n",
        "recall = recall_score(val_df['toxicity'], val_pred, average='macro')\n",
        "precision = precision_score(val_df['toxicity'], val_pred, average='macro')\n",
        "f1 = f1_score(val_df['toxicity'], val_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print('Accuracy:', accuracy)\n",
        "print('Recall (macro):', recall)\n",
        "print('Precision (macro):', precision)\n",
        "print('F1 Score (macro):', f1)\n",
        "\n",
        "# Compute and print confusion matrix\n",
        "labels = np.unique(np.concatenate((val_df['toxicity'], val_pred)))\n",
        "cm = confusion_matrix(val_df['toxicity'], val_pred, labels=labels)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMCO71B9-shm",
        "outputId": "4f94f4d9-c612-46a8-c3ae-0a1a02d43273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8684931506849315\n",
            "Recall (macro): 0.49980291683090267\n",
            "Precision (macro): 0.434395340870161\n",
            "F1 Score (macro): 0.46480938416422285\n",
            "Confusion Matrix:\n",
            "[[2536    1]\n",
            " [ 383    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training data and transform the training data\n",
        "train_features = vectorizer.fit_transform(train_df['preprocessed_comment'])\n",
        "\n",
        "# Transform the validation data using the fitted vectorizer\n",
        "val_features = vectorizer.transform(val_df['preprocessed_comment'])\n",
        "\n",
        "# Create an instance of XGBClassifier with the best parameters\n",
        "model = XGBClassifier(tree_method='hist')\n",
        "\n",
        "# Fit the model on the training features and labels\n",
        "model.fit(train_features, train_df['toxicity'])\n",
        "\n",
        "# Predict on the validation features\n",
        "val_pred = model.predict(val_features)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(val_df['toxicity'], val_pred)\n",
        "recall = recall_score(val_df['toxicity'], val_pred, average='macro')\n",
        "precision = precision_score(val_df['toxicity'], val_pred, average='macro')\n",
        "f1 = f1_score(val_df['toxicity'], val_pred, average='macro')\n",
        "\n",
        "# Print metrics\n",
        "print('Accuracy:', accuracy)\n",
        "print('Recall (macro):', recall)\n",
        "print('Precision (macro):', precision)\n",
        "print('F1 Score (macro):', f1)\n",
        "\n",
        "# Compute and print confusion matrix\n",
        "labels = np.unique(np.concatenate((val_df['toxicity'], val_pred)))\n",
        "cm = confusion_matrix(val_df['toxicity'], val_pred, labels=labels)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Perform feature selection with RFE\n",
        "num_features_to_keep = 1500  # Adjust this value as per your requirements\n",
        "rfe = RFE(estimator=model, n_features_to_select=num_features_to_keep)\n",
        "\n",
        "# Fit RFE on the training features and labels\n",
        "rfe.fit(train_features, train_df['toxicity'])\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = train_features[:, rfe.support_]\n",
        "\n",
        "# Retrain the model using the selected features\n",
        "model.fit(selected_features, train_df['toxicity'])\n",
        "\n",
        "# Predict on the validation features using the model trained with selected features\n",
        "val_features_selected = val_features[:, rfe.support_]\n",
        "val_pred_selected = model.predict(val_features_selected)\n",
        "\n",
        "# Calculate metrics on the selected features\n",
        "accuracy_selected = accuracy_score(val_df['toxicity'], val_pred_selected)\n",
        "recall_selected = recall_score(val_df['toxicity'], val_pred_selected, average='macro')\n",
        "precision_selected = precision_score(val_df['toxicity'], val_pred_selected, average='macro')\n",
        "f1_selected = f1_score(val_df['toxicity'], val_pred_selected, average='macro')\n",
        "\n",
        "# Print metrics on the selected features\n",
        "print('Accuracy (selected features):', accuracy_selected)\n",
        "print('Recall (macro) (selected features):', recall_selected)\n",
        "print('Precision (macro) (selected features):', precision_selected)\n",
        "print('F1 Score (macro) (selected features):', f1_selected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "0WWu2Y2g_5yn",
        "outputId": "9da4830f-d9b6-409f-f4ee-c1694f1f4351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8664383561643836\n",
            "Recall (macro): 0.4986204178163185\n",
            "Precision (macro): 0.4342602128389976\n",
            "F1 Score (macro): 0.46422018348623845\n",
            "Confusion Matrix:\n",
            "[[2530    7]\n",
            " [ 383    0]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7812f866160f>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Fit RFE on the training features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'toxicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Get the selected features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;31m# Get importance and rank them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSnfsGmyc5yb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}