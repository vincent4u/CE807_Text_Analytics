{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/CE807_Text_Analytics/blob/main/week2/lab02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTBCtz9oS18L"
      },
      "source": [
        "# Text Processing and Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6njrgD-TFnx"
      },
      "source": [
        "## Normalizing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rpfdIlnMSR11"
      },
      "outputs": [],
      "source": [
        "text = \"Hey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP! @AmazonHelp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuRer7lvTKtB",
        "outputId": "53104596-af5e-45cd-e1d5-5cfaa1d55ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\n"
          ]
        }
      ],
      "source": [
        "text = text.lower()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61FpGyRTR8p"
      },
      "source": [
        "## Removing Unicode Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qQSZwOW2TQvF"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILnp8eg3TYmF",
        "outputId": "d3712f2a-7580-4a9e-8593-0b9b5ef30339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey amazon  my package never arrived  please fix asap amazonhelp\n"
          ]
        }
      ],
      "source": [
        "text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlIUCuIHzH2x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiLcjm3mwBYj"
      },
      "source": [
        "**Your Turn: 1. remove only URL. 2. remove only numbers. 3. remove only special characters like $, @ etc. Do this for given text, try with different text inputs**\n",
        "\n",
        "Hint: https://docs.python.org/3/howto/regex.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE-ssmWYTfNW"
      },
      "source": [
        "## Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnzouVKkTgXv",
        "outputId": "9b033d72-86c4-4d21-9a68-1804533d0253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfSkjvknTi-Y",
        "outputId": "73a2f34a-a068-4a8a-a307-8fc00e998ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package amazon never arrived fix asap\n"
          ]
        }
      ],
      "source": [
        "stop = stopwords.words('english')\n",
        "text = \"my package from amazon never arrived fix this asap\"\n",
        "text = \" \".join([word for word in text.split() if word not in (stop)])\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPRsMsT5Ts5h"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_yCGEMUT2Mp"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OtqAr97HTn5P"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDS0mciTyyI",
        "outputId": "af854b9c-b8c7-4bd9-a9c8-77ca01fc3f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jump = jump\n",
            "jumped = jump\n",
            "jumps = jump\n",
            "jumping = jump\n"
          ]
        }
      ],
      "source": [
        "words = [\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
        "stemmer = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word + \" = \" + stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGEMUT1OT3-p"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUfxpb6kT0_u",
        "outputId": "d497005b-100f-4563-c462-b37ec650b90b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHMSwuyJUAL5",
        "outputId": "ee0fc1cc-bd48-40a6-ec5e-527c195580fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jump = jump\n",
            "jumped = jumped\n",
            "jumps = jump\n",
            "jumping = jumping\n"
          ]
        }
      ],
      "source": [
        "words = [\"jump\", \"jumped\", \"jumps\", \"jumping\"]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in words:\n",
        "  print(word + \" = \" + lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-axuriWMVgMp"
      },
      "source": [
        "Play with different Stemming and Lemmatization examples and algorithms, and build intuition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azjw6DO_UK7J"
      },
      "source": [
        "## Part of Speech (POS) Tagging\n",
        "\n",
        "There are eight main parts of speech, and using NLTK to tag each within our data allows us to glean further useful insight from our text.\n",
        "\n",
        "For instance, by tagging and grouping our adjectives, we can calculate the most and least used descriptors, which points us towards our productsâ€™ strengths and weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REBWgnE3UB5f",
        "outputId": "13d7d592-099b-4041-a039-560a64d779b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6Tra38qUT62",
        "outputId": "2e247e89-933f-4589-e1ff-fa29b8c340d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amazon', 'package', 'never', 'arrived', 'fix', 'asap']\n"
          ]
        }
      ],
      "source": [
        "tokens = nltk.word_tokenize(\"amazon package never arrived fix asap\")\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgBLYNUqUWKx",
        "outputId": "a9561134-ff78-4bc7-cf30-4ebb635a4b39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8vcUWr2UZtI",
        "outputId": "4bbc4de8-2700-4da4-b482-3eaf6c22d5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('amazon', 'JJ'), ('package', 'NN'), ('never', 'RB'), ('arrived', 'VBD'), ('fix', 'JJ'), ('asap', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "tokens = ['amazon', 'package', 'never', 'arrived', 'fix', 'asap']\n",
        "pos = nltk.pos_tag(tokens)\n",
        "\n",
        "print(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxPOEc61V1at"
      },
      "source": [
        "**Your turn: Identify different POS tags and think about which one you think would be useful.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT_Zvj4oUcyg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBV-IjZkWSS5"
      },
      "source": [
        "## Text Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEjpMvxhWV3p"
      },
      "source": [
        "How to represent a corpus using one hot encoding using sklearn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7PRn_TpXmoW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGl9LMXxXnzM"
      },
      "outputs": [],
      "source": [
        "corpus = [\"How to format my hard disk problem\", \" Hard disk format problems \", \"My amazon review is available at\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0qbDWAfYBWi"
      },
      "source": [
        "We need to get the words in all sentences. We could perform different pre-processing step, however we are not going to do that here.\n",
        "\n",
        "Easiest way to get words in a sentence is to split by space, let's do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoDKGibqXgS"
      },
      "source": [
        "### Bag of words representation\n",
        "\n",
        "You already saw this in the last lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb9Hf5IBqngi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro934vZFqpIy"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=['to', 'at'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkxsrt_1AwRE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOYnCD_hqci9"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUyvMpICqsGq",
        "outputId": "abd5ab8d-c085-49b8-f1fb-a8821a38a81f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['amazon', 'available', 'disk', 'format', 'hard', 'how', 'is', 'my',\n",
              "       'problem', 'problems', 'review'], dtype=object)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbTWijeR95Hg",
        "outputId": "8f2882c5-041a-40cc-d2bf-225882252e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D96MWtwWq3ZC"
      },
      "source": [
        "Let's get Bag of word represemtation of `corpus` sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y333qH9oX8vN",
        "outputId": "06fe6706-0b17-47df-aebe-e13ce2b9271b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKLZ5F9b-SxI"
      },
      "outputs": [],
      "source": [
        "test = [' testing bow representation amazon amazon']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VoqwiLK-tdw"
      },
      "outputs": [],
      "source": [
        "y = vectorizer.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNP5SKTK-3ff",
        "outputId": "1a6a3757-672c-4883-9f03-ab07f8bb98a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtbdKBAa978f",
        "outputId": "3422bb19-de69-457a-ae9a-037889bb3f2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 11)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.toarray().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRMKaxrMq1YS"
      },
      "source": [
        "CountVectorizer has a number of very useful options, discussed at the page:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "you should spend some time now familiarizing yourself with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVtPmYvKr8Ay"
      },
      "source": [
        "**Your turn:  Take some test sentences and create it's bag of word representations.**\n",
        "\n",
        "**Your turn:  How you will deal with the unknow words which was not seen during the vectorizer training time.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgQCjo9OrKbD"
      },
      "source": [
        "### tf-idf based representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69bdUQv2rTf9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwtLRnfErYFs"
      },
      "source": [
        "Creating an instance of TfidfVectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOf-nCcJrUg6"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6abo14IrbMz"
      },
      "source": [
        "Letâ€™s transform the data now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pda3ZHzdreBJ"
      },
      "outputs": [],
      "source": [
        "transformed = tfidf.fit_transform(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_TKtJrfrskE"
      },
      "source": [
        "**Your turn: Find way to get word/token names, and view tf-idf representation of the `cropus`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG_hwhj4sUX_"
      },
      "source": [
        "### One hot vector representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQrbIQGAsZJ0"
      },
      "source": [
        "**Your turn:  Use hot vector based representation of words to represent any sentence** You might need to use `OneHotEncoder` and `LabelEncoder`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knvfTyFDtKfs"
      },
      "source": [
        "### word2vec Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7J6ZoHutN6n"
      },
      "source": [
        "We will see how to load and do some basic operations using word2vec representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkIwhQG8txMU"
      },
      "source": [
        "There are many word2vec models available; we will use\n",
        "Gensim one. We will have to download word2vec, which takes time (2GB download). So while\n",
        "it is downloading, we could play with the online demo and try to understand how word2vec\n",
        "works.\n",
        "\n",
        "Demos:\n",
        "\n",
        "*   https://turbomaze.github.io/word2vecjson/\n",
        "*   http://epsilon-it.utu.fi/wv_demo/\n",
        "*   http://nlp.polytechnique.fr/word2vec\n",
        "*   http://vectors.nlpl.eu/explore/embeddings/en/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf_1nEo9eISI"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrWyNGI1ycbu",
        "outputId": "95202d89-d95e-4f9f-b752-d857773b83ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.5% 1637.2/1662.8MB downloaded"
          ]
        }
      ],
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsYPcb4ZuEEg"
      },
      "source": [
        "**Your Turn: Find out how to save and load word2vec model, so that you don't need to download is again and again**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsucsis5uNOX"
      },
      "source": [
        "Let's retrieve the vocabulary of a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXxzZsX_uA4L",
        "outputId": "47582842-090e-4c3f-aa72-6c69413f75bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/3000000 is </s>\n",
            "word #1/3000000 is in\n",
            "word #2/3000000 is for\n",
            "word #3/3000000 is that\n",
            "word #4/3000000 is is\n",
            "word #5/3000000 is on\n",
            "word #6/3000000 is ##\n",
            "word #7/3000000 is The\n",
            "word #8/3000000 is with\n",
            "word #9/3000000 is said\n",
            "word #10/3000000 is was\n",
            "word #11/3000000 is the\n",
            "word #12/3000000 is at\n",
            "word #13/3000000 is not\n",
            "word #14/3000000 is as\n",
            "word #15/3000000 is it\n",
            "word #16/3000000 is be\n",
            "word #17/3000000 is from\n",
            "word #18/3000000 is by\n",
            "word #19/3000000 is are\n",
            "word #20/3000000 is I\n",
            "word #21/3000000 is have\n",
            "word #22/3000000 is he\n",
            "word #23/3000000 is will\n",
            "word #24/3000000 is has\n",
            "word #25/3000000 is ####\n",
            "word #26/3000000 is his\n",
            "word #27/3000000 is an\n",
            "word #28/3000000 is this\n",
            "word #29/3000000 is or\n",
            "word #30/3000000 is their\n",
            "word #31/3000000 is who\n",
            "word #32/3000000 is they\n",
            "word #33/3000000 is but\n",
            "word #34/3000000 is $\n",
            "word #35/3000000 is had\n",
            "word #36/3000000 is year\n",
            "word #37/3000000 is were\n",
            "word #38/3000000 is we\n",
            "word #39/3000000 is more\n",
            "word #40/3000000 is ###\n",
            "word #41/3000000 is up\n",
            "word #42/3000000 is been\n",
            "word #43/3000000 is you\n",
            "word #44/3000000 is its\n",
            "word #45/3000000 is one\n",
            "word #46/3000000 is about\n",
            "word #47/3000000 is would\n",
            "word #48/3000000 is which\n",
            "word #49/3000000 is out\n"
          ]
        }
      ],
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 50: # For simplicity, we are looking at 50 word only\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiKnq2nTuWBR"
      },
      "source": [
        "Let's get Vector representation of a word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZs9qvDHuagT"
      },
      "outputs": [],
      "source": [
        "word = 'king'\n",
        "vec_king = wv[word]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgR2DHsgubXS"
      },
      "source": [
        "Let's calculate Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eUNQyYFufyB",
        "outputId": "41245491-40ff-4f13-9722-7a4a75cc4ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'car'\t'minivan'\t0.69\n",
            "'car'\t'bicycle'\t0.54\n",
            "'car'\t'airplane'\t0.42\n",
            "'car'\t'cereal'\t0.14\n",
            "'car'\t'communism'\t0.06\n"
          ]
        }
      ],
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrMThOGJugbR"
      },
      "source": [
        "Let's find nearest words to give words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUyu4iZbuk6b",
        "outputId": "200fb361-3c29-4a4d-c52d-c2b5c274cdb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474)]\n"
          ]
        }
      ],
      "source": [
        "words = ['car', 'minivan']\n",
        "print(wv.most_similar(positive= words, topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvcW26JZyghp"
      },
      "source": [
        "**Your Turn: Play with different words and build intuition about the representation**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}