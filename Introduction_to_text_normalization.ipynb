{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOorMafHw4r5UMk4PTViFes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincent4u/CE807_Text_Analytics/blob/main/Introduction_to_text_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SlbBizsDgl85"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Normalization**\n",
        "The process of converting text into a standard, consistent format. It involves transforming text in ways such as removing punctuation, converting to lowercase, expanding contractions, and handling special characters or symbols. Here are the steps to perform text normalization:"
      ],
      "metadata": {
        "id": "WB05_L8xgoOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1.0 Remove punctuations& special characters,numbers,URL's and return lower case ***"
      ],
      "metadata": {
        "id": "w5MDL8569BfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. Converting to lowercase:***\n",
        "This is an important step to ensure consistency."
      ],
      "metadata": {
        "id": "As_P2iIzg5zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello World!\"\n",
        "normalized_text = text.lower()  # the actual code iks the .lower()\n",
        "print(normalized_text)          #printing out the result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql3O3vNxg1df",
        "outputId": "8d83f235-b42e-4903-ff28-65e9154c7ada"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Import re*** :\n",
        "we'd be using the regular expression or regex function on python to remove punctuations, remove numbers, url and handling special characters\n",
        "\n",
        "***2. Removing puctuations and special characteres*** : it is also uimportant to remove puctuation marks and special characters like commas, period, exclamation marks (\"* % ^ & #, @ © ® ± ≠ ¥ € £ !\")etc.Punctuation marks often do not carry significant meaning in many natural language processing tasks."
      ],
      "metadata": {
        "id": "co1awpfuhizX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello_1*%^& #, @World  ©®±≠ ¥ ¥€£ !\"\n",
        "normalized_text = re.sub(r'[^\\w\\s]', '', text)        # r'[^\\w\\s] removes anything that is not a word or whitespace (like * , % ^ & #, @ © ® ± ≠ ¥ € £ ! .) and re.sub replaces them with an empty space\n",
        "print(normalized_text)                                # note that letters, digits and underscore_ are exempted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQqgzPmFiArX",
        "outputId": "dfc2db26-b0c5-4a71-f0c4-5f86b4a5442f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello_1  World     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively we can use this\n",
        "import re\n",
        "text= \"Hello_1*%^& #, World!\"\n",
        "normalized_text= re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "print(normalized_text)\n",
        "\n",
        "# both r'[^\\w\\s]' and r'[^a-zA-Z0-9\\s]' are the same and can be used interchagebly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri0ugKbCn8-n",
        "outputId": "7259589e-e4e2-4b47-931c-e33e72ddccb2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello1  World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3. Handling numbers:***\n",
        "we can choose to remove numbers,  replace them with # hashtags or convert them ton words\n"
      ],
      "metadata": {
        "id": "MmkPB3DiuPkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing numbers\n",
        "\n",
        "import re\n",
        "text = \"I have 5 apples and 3 oranges.\"\n",
        "normalized_text = re.sub(r'\\d+', '', text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Knfe2fuwRX",
        "outputId": "e4112245-48a0-47da-e07b-e04a62285d9f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have  apples and  oranges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***4. Removing URLs:***\n"
      ],
      "metadata": {
        "id": "8yqD2bRQwOcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Check out my website at https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first for more information PLEASE FIX ASAP! @AmazonHelp\"\n",
        "\n",
        "normalized_text = re.sub(r'https\\S+', '', text)  # note that this is only removing the url and not addressing special charcters, puntuatuion and or numbers. note also the S is capital and its different from otheers\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPxPXD0YwckO",
        "outputId": "01b4cf80-c0fc-49ce-ce71-260161596f79"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check out my website at https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first for more information PLEASE FIX ASAP! @AmazonHelp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining all 4 regex function in one code line together\n",
        "***converting lower case and removing  punctuations, numbers,special characters urls and ***"
      ],
      "metadata": {
        "id": "DNGEb5-j1wnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text_input= \"Check out my website at https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first for more information contact me. My 5 IPHONE WATCHES i ordered for with 3£ @ a store on aMAZON has not arrived PLEASE FIX ASAP! @AmazonHelp  #twitterhelp,  ©telecom,  ®LMTD ±giveand take\"\n",
        "normalized_text_input= re.sub(r'[^\\w\\s]|https?\\S+|\\d+',' ',text_input).lower()\n",
        "print(normalized_text_input)\n",
        "\n",
        "# note this code snipet removes numbers, punctuaion marks, special characters, URL's and conversts all to lower case:  re.sub(r'[^\\w\\s]|https?\\S+|\\d+',' ',text_input).lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEf8wD4i2QnV",
        "outputId": "42169a20-eba3-4f45-9472-a4f5761b0383"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check out my website at   for more information contact me  my   iphone watches i ordered for with      a store on amazon has not arrived please fix asap   amazonhelp   twitterhelp    telecom    lmtd  giveand take\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.0 Expand contractions:**\n",
        "\n",
        "Expanding contractions to their full word form is very important. For example, convert \"can't\" to \"cannot,\" \"won't\" to \"will not,\" and \"I'm\" to \"I am.\" This step helps in standardizing the text and reducing variations.\n",
        "\n",
        "don't forget to pip install contractions"
      ],
      "metadata": {
        "id": "-1JR7pvA9iJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glewMaIa-ZnM",
        "outputId": "16ce617d-ed5f-49d1-e95a-241552b7db1b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "text = \"I can't wait!\"\n",
        "normalized_text = contractions.fix(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZemmcgG9X66",
        "outputId": "ebfe2f15-7407-4eaf-8b6e-37b41d68c313"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I cannot wait!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.0 Tokenization, Removing stop words**\n",
        "\n",
        "It is generally recommended to perform tokenization before removing stopwords and applying lemmatization or stemming techniques.\n",
        "Tokenization is the process of breaking text into individual words or tokens, which serves as the foundation for further text processing tasks"
      ],
      "metadata": {
        "id": "TsAOeOOpAQgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokenization and text_splitting***\n",
        "Tokenization is a way of breaking text into smaller tokens. Its similar to text_splitting bbut with some slight differnces. Be mindfull of the differences"
      ],
      "metadata": {
        "id": "MARTF1SlBqaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text_splitting\n",
        "\n",
        "text= 'is it your birthday_girl today?'\n",
        "normalized_text_split= text.split()\n",
        "print(normalized_text_split)  #notice the underscore_  and the question mark? wastn't seperated.     see the difference in tokkenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3DkyVCOA1lE",
        "outputId": "7dbebfdc-b64e-4b13-99cb-447b3db16a39"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'it', 'your', 'birthday_girl', 'today?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')                                          #if you intend to tokenize remeber to import nltk and from nltk.tokenize import word_tokenize. you may not need to download 'punkt\n",
        "from nltk.tokenize import word_tokenize                         # there is also sentence tokinze ie. sent_tokenize. see next one\n",
        "\n",
        "#text = \"Hello, how are you today?\"\n",
        "text= 'is it your birthday_girl today?'\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)                                                   #notice the underscore_   wastn't seperated but the questionmarks was. with tokenization punctuations and special characters are seperated as a token.\n",
        "                                                                # however to return it back to sentence format you use the ''.join"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlmBEqd0Cs6p",
        "outputId": "d9c6dbdd-b0cc-4a16-8e54-11adbf18a064"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'it', 'your', 'birthday_girl', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text='is it your birthday_girl today?'\n",
        "token=word_tokenize(text)\n",
        "print(f' this if for tokenizer',token)\n",
        "\n",
        "split_word=text.split()\n",
        "print(f' this is for word_spliting', split_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN-_oXNmL-KQ",
        "outputId": "f92f1114-9ef0-49bf-b5e8-8972878e2fcf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " this if for tokenizer ['is', 'it', 'your', 'birthday_girl', 'today', '?']\n",
            " this is for word_spliting ['is', 'it', 'your', 'birthday_girl', 'today?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "text = \"This is the first sentence. The second sentence is here. Another sentence follows. One more sentence to go.\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(f' this is for sentence_tokenize', sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WteRqyLKGD8f",
        "outputId": "7f2eae56-d9fa-445e-ad0b-14296e1eb7b3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " this is for sentence_tokenize ['This is the first sentence.', 'The second sentence is here.', 'Another sentence follows.', 'One more sentence to go.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Removing stop_words***\n",
        "\n",
        "Stop words are commonly used words in a language that do not carry much meaning, such as \"a,\" \"an,\" \"the,\" \"is,\" \"are,\" etc. Removing these words can help reduce noise and focus on more important content. However, the removal of stop words depends on the specific task and context.\n",
        "\n",
        "Be mindful, stop_words are not punctuation marks."
      ],
      "metadata": {
        "id": "edc_lBl3HNOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "#alternatively\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#lets remove stop_words from a single sentence\n",
        "text=\"hey! helloo, is this the first sentence?.\"\n",
        "stop_words= set(stopwords.words('english'))\n",
        "\n",
        "#utilizing tokenizer\n",
        "tokens = word_tokenize(text)\n",
        "normalized_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "normalized_text = \" \".join(normalized_tokens)\n",
        "print(normalized_text)\n",
        "\n",
        "#utilizing splitting\n",
        "text = \"my package from amazon never arrived fix this asap\"\n",
        "text = \" \".join([word for word in text.split() if word not in (stop_words)])\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3rYt6xjHTCW",
        "outputId": "be133bd5-8ce3-44cf-fffc-15652fa8b3d6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey ! helloo , first sentence ? .\n",
            "package amazon never arrived fix asap\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Removing stop words utilizing tokenizers***"
      ],
      "metadata": {
        "id": "sQ1--cx_R2xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "                                                                          #this is with tokenization\n",
        "stop_words=set(stopwords.words('english'))\n",
        "text=\"hey! helloo, is this the first sentence?.\"\n",
        "tokenize= word_tokenize(text)\n",
        "new_word=[token for token in tokenize if token.lower() not in stop_words]\n",
        "normalized_word=''.join(new_word)\n",
        "print(normalized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uivExf-1IWOR",
        "outputId": "d6d28a88-4a79-4328-dd71-482d57e6db50"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey!helloo,firstsentence?.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Removing stop words utilizing .split() ie. splitting***"
      ],
      "metadata": {
        "id": "HhQ_XmR-SGV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "                                                                                #this is with splitting\n",
        "stop_words=set(stopwords.words('english'))\n",
        "text=\"hey! helloo, is this the first sentence?.\"\n",
        "new_word=[word for word in text.split() if word not in (stop_words)]\n",
        "normalized_words2=''.join(new_word)\n",
        "print(normalized_words2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAdiuHtFP_q2",
        "outputId": "cf23fd40-c175-4bac-96dc-2a8b793b3a8a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey!helloo,firstsentence?.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization and stemming**\n",
        "\n",
        " Apply lemmatization or stemming techniques to reduce words to their base or root form. Lemmatization converts words to their base form (lemma), while stemming truncates words to their root form. For example, \"running,\" \"runs,\" and \"ran\" would all be reduced to the base form \"run.\" This step helps in treating similar words as the same, which can be beneficial for tasks such as information retrieval or sentiment analysis.\n",
        "\n",
        "\n",
        "note: you have to first tokenize befor attempting to lemmatize"
      ],
      "metadata": {
        "id": "JUfNGkDlSSwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "text = \"running runs ran\"\n",
        "lemmatizer = WordNetLemmatizer()       #setting the lemmatizer\n",
        "porter_stemmer = PorterStemmer()        #setting the stemmer\n",
        "\n",
        "tokens = word_tokenize(text)            #tokenizingt the text\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(lemmatized_tokens)\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xczC4Dy1SX70",
        "outputId": "11764d4d-4fd5-4393-bf1d-e13d7b1073bf"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'run', 'ran']\n",
            "['run', 'run', 'ran']\n"
          ]
        }
      ]
    }
  ]
}